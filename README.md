# Revisiting CoNNL-2003 with Classical Machine Learning
Contributors: Matthew A Hernandez
> **Note** Access to our paper [here](https://github.com/weezymatt/Retrieval-with-Wordle/blob/main/Retrieval-with-Wordle.pdf)

Last updated November 21st, 2024

> **Note** The scope of this project involves the following points: establish multiple baselines to support machine learning methods and provide a survey of classical machine learning to motivate more complex modelling (e.g., CRFs and LSTMs) for NER.

This project was created for the purpose of applying techniques in Machine Learning, built from scratch, to develop a Named Entity Recognition system. The main comparision is between a first-order Hidden Markov Model and a Maximum Entropy Markov Model.

> **Note 2** The Hidden Markov Model and all decoding strategies are built from scratch except for the Logistic Regression classifier for the MEMM. Seq-to-seq modelling is a demanding task and efficient libraries carry out experiments in a timely manner. A working toy classifier is built in the *models* folders.

## Table of Contents
- [Objective](#objective)
- [Virtual Environment](#virtual-environment)
- [Hidden Markov Model](#hidden-markov-model)
- [Maximum Entropy Markov Model](#maximum-entropy-markov-model)
- [Baselines & Benchmarks](#baselines-&-benchmarks)

## Objective

## Virtual Environment

## Hidden Markov Model

## Maximum Entropy Markov Model

## Baselines & Benchmarks
