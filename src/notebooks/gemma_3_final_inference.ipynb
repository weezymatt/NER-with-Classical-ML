{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN2mp3M61Cl20n8VsIZoWJ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weezymatt/NER-with-Classical-ML/blob/main/src/notebooks/gemma_3_final_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma 3 — Inference"
      ],
      "metadata": {
        "id": "7cXV7sA7Bsyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Framework/ Library Installation"
      ],
      "metadata": {
        "id": "-nMAoD0RBzao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Installs unsloth and other dependencies optimized for colab\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --upgrade transformers accelerate bitsandbytes datasets asteval GPUtil"
      ],
      "metadata": {
        "id": "UeQy_ccMBuFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import re\n",
        "import pdb\n",
        "\n",
        "from tqdm import tqdm\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "xKXYXTclCnbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use helper functions from repository\n",
        "\n",
        "colab = True\n",
        "if colab:\n",
        "  if not os.path.exists(\"info621/\"):\n",
        "    !git clone https://github.com/srikrish2812/info621_project info621"
      ],
      "metadata": {
        "id": "ce-dgHZgZZWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the final model and dataset"
      ],
      "metadata": {
        "id": "SVzn3GuWMYEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "  \"\"\"\n",
        "  Load the trained Gemma 3 1B with Unsloth.\n",
        "  \"\"\"\n",
        "  model_name = \"abhay2812/gemma-3-1b-4bit-grpo\"\n",
        "  model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "      model_name,\n",
        "      load_in_4bit=True,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "  FastLanguageModel.for_inference(model)\n",
        "\n",
        "  return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model()"
      ],
      "metadata": {
        "id": "MfkCjYtzMr6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from info621.src.tasks.gsm8k import GSM8kTask\n",
        "# from gsm8k import GSM8kTask\n",
        "\n",
        "\n",
        "gsm8k = GSM8kTask()\n",
        "dataset = gsm8k.get_questions()"
      ],
      "metadata": {
        "id": "iiOWsOWnaeZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "- Run inference with final model on random sample from the test dataset."
      ],
      "metadata": {
        "id": "Jf4xl6RXYdaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer"
      ],
      "metadata": {
        "id": "NrREca3hNNet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset['train'].select([0])\n",
        "sample['prompt']"
      ],
      "metadata": {
        "id": "hS0xi8dIETRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(sample, measure_p=False, max_new_tokens=256):\n",
        "  text = tokenizer.apply_chat_template(\n",
        "      sample,\n",
        "      add_generation_prompt = True,\n",
        "      tokenize = False,\n",
        "  )\n",
        "  streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "  if measure_p:\n",
        "    streamer = None\n",
        "\n",
        "  tensor = model.generate(\n",
        "      **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "      max_new_tokens = max_new_tokens,\n",
        "      temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "      streamer = streamer,\n",
        "  )\n",
        "  return tensor"
      ],
      "metadata": {
        "id": "G4SdR2fRyA5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output = run_model(sample['prompt'], measure_p=False)"
      ],
      "metadata": {
        "id": "Q5jbbsjKEOEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output = tokenizer.decode(model_output[0])\n",
        "y = gsm8k.extract_answer(decoded_output)\n",
        "sample['answer'][0] == y"
      ],
      "metadata": {
        "id": "dB8fnsmMradH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Code demonstration finished!\")"
      ],
      "metadata": {
        "id": "IDTukc1ycq0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measure Gemma 3 Performance — Latency  on NVIDIA® T4 GPU\n",
        "Latency refers to the time it takes for a model to output a response based on the input. The output of LLMs has a couple possibilities: streaming or non-streaming mode. They effectively impact user experience and are crucial in developing AI applications.\n",
        "\n",
        "There are a few key metrics that define latency, that differ between streaming and non-streaming modes.\n",
        "\n",
        "1. Time to first token (TTFT): The TTFT represents how your application starts responding. It's the amount of time from when the user submits a query until a certain threshold is reached (i.e., first token, word, or chunk). Other variants include Time to last token (TTLT). The response time is affected by several factors:\n",
        "  - Length of input prompt\n",
        "  - Network conditions and geographic locations\n",
        "  - **Calculation:** Time to first token - Time from query submission\n",
        "  - Interpretation: lower is better\n",
        "\n",
        "2. End-to-end latency (E2E): E2E latency measures the overall time time to complete the response. Key factors that impact the response time:\n",
        "  - Length of input prompt\n",
        "  - Requested amount length\n",
        "  - Maximum amount of tokens the model produces\n",
        "  - Complexity of the task\n",
        "  - **Calculation:** Time at completion of request - Time from query submission\n",
        "  - Interpretation: lower is better.\n",
        "\n",
        "Warmup is shown to improve latency, therefore we each metric is evaluated with warmup.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j5Ehi981vsw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import GPUtil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "96CIycNAGpZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_latency(sample, iterations=1, warmup=False, max_new_tokens=256):\n",
        "  if warmup:\n",
        "    for _ in range(5):\n",
        "      random_sample = gsm8k.__getsamples__(n_samples=1, split=\"train\")\n",
        "      _ = run_model(random_sample['prompt'], measure_p=True, max_new_tokens=max_new_tokens)\n",
        "\n",
        "  latencies = []\n",
        "  gpus = GPUtil.getGPUs()\n",
        "\n",
        "  if not gpus:\n",
        "    raise ValueError(\"No GPUs found.\")\n",
        "  gpu = gpus[0]\n",
        "\n",
        "  for _ in range(iterations):\n",
        "    start_time = time.time()\n",
        "    _ = run_model(sample['prompt'], measure_p=True, max_new_tokens=max_new_tokens)\n",
        "    end_time = time.time()\n",
        "    latencies.append(end_time-start_time)\n",
        "\n",
        "  return latencies"
      ],
      "metadata": {
        "id": "Q06FMwVy1Hvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_eval_latency(latencies, iterations=100):\n",
        "  print(f\"Average latencies per {iterations} iterations: {np.mean(latencies):.4f} seconds\")\n",
        "  print(f\"Maximum latency per {iterations} iterations: {np.max(latencies):.4f} seconds\")\n",
        "  print(f\"Minimum latency per {iterations} iterations: {np.min(latencies):.4f} seconds\")"
      ],
      "metadata": {
        "id": "3HPoU37vD3wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_milliseconds(vector):\n",
        "  return np.array(vector) * 1000"
      ],
      "metadata": {
        "id": "_iiAdICdNULR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the interval of tokens length in the training set of the dataset.\n",
        "- The baseline number of tokens is 20.\n",
        "- The interval is between 44 and 245 tokens.\n",
        "- There will be 30 trials (iterations) for each token length with a step size of 25 tokens."
      ],
      "metadata": {
        "id": "sjGUIn60TLrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer_(x):\n",
        "  return {\"input_ids\": tokenizer.apply_chat_template(x['prompt'], add_generation_prompt=True,tokenize=True)}\n",
        "\n",
        "def length(x):\n",
        "  return {\"length\": len(x['input_ids'])}"
      ],
      "metadata": {
        "id": "NiJANeGoUKYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_map = dataset.map(tokenizer_)\n",
        "dataset_map = dataset_map.map(length)\n",
        "pd_dataset = dataset_map['train'].to_pandas()"
      ],
      "metadata": {
        "id": "rPT3zGuVTXj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intervals = [44, 70, 95, 120, 145, 170, 194, 219, 245] # token length intervals"
      ],
      "metadata": {
        "id": "o0rMAUB9XOLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = []\n",
        "for interval in intervals:\n",
        "  sample_space = pd_dataset[pd_dataset['length'] == interval]\n",
        "  indices.append(sample_space.sample().index[0])"
      ],
      "metadata": {
        "id": "bBi_3hUFbYzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Measure baseline latencies and evaluate."
      ],
      "metadata": {
        "id": "Y9Bz1q4p7SS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "measure_latency = True\n",
        "iterations = 30"
      ],
      "metadata": {
        "id": "TM3FdUxAD1rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_prompt = \"Hello, world.\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": baseline_prompt}\n",
        "]\n",
        "\n",
        "baseline_msg = {\"prompt\": messages}"
      ],
      "metadata": {
        "id": "jVs6ILreARRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if measure_latency:\n",
        "  print(\"BASELINE_TTFT_WARMUP\")\n",
        "  baseline_ttft_warmup = test_latency(baseline_msg, iterations=iterations, warmup=True, max_new_tokens=1)\n",
        "  test_eval_latency(baseline_ttft_warmup)"
      ],
      "metadata": {
        "id": "Ua37AuxdF1x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if measure_latency:\n",
        "  print(\"BASELINE_E2E_WARMUP\")\n",
        "  baseline_e2e_warmup = test_latency(baseline_msg, iterations=iterations, warmup=True, max_new_tokens=256)\n",
        "  test_eval_latency(baseline_e2e_warmup)"
      ],
      "metadata": {
        "id": "xey1Zqm5F8u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Measure time to first token and evaluate."
      ],
      "metadata": {
        "id": "ACWGITqf-Cbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if measure_latency:\n",
        "  latencies_ttft = {}\n",
        "  for idx in tqdm(indices):\n",
        "    random = dataset['train'].select([idx])\n",
        "    ttft_warmup = test_latency(random, iterations=iterations, warmup=True, max_new_tokens=1)\n",
        "    latencies_ttft[idx] = {\"warmup\": ttft_warmup}"
      ],
      "metadata": {
        "id": "ejGPRA9ic4lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "for i,v in latencies_ttft.items():\n",
        "  data.append(convert_to_milliseconds(v['warmup']))"
      ],
      "metadata": {
        "id": "qFDxnauneUDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a line plot to visualize the TTFT latency."
      ],
      "metadata": {
        "id": "9SeU7J-J8GlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_boxplot(data, y, configs):\n",
        "  fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "  ax.set_title(configs['title'])\n",
        "  ax.set_xlabel(configs['xlabel'])\n",
        "  ax.set_ylabel(configs['ylabel'])\n",
        "  ax.yaxis.grid(color='white')\n",
        "  ax.set_facecolor(color='gainsboro')\n",
        "  bp = ax.boxplot(data, patch_artist=True, tick_labels=y, boxprops=configs['boxprops'])\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(configs['png'])\n",
        "  return plt.show()"
      ],
      "metadata": {
        "id": "W_zhl4M3F57g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ttft_configs = {}\n",
        "ttft_configs['title'] = 'Gemini 1B 4bit Latency for Time to First Token (TTFT)'\n",
        "ttft_configs['xlabel'] = f\"Prompt Tokens with {iterations} Trials\"\n",
        "ttft_configs['ylabel'] = 'Latency (milliseconds)'\n",
        "ttft_configs['boxprops'] = dict(facecolor=\"tab:blue\")\n",
        "ttft_configs['png'] = 'ttft.png'"
      ],
      "metadata": {
        "id": "yZnfhWG-7cwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_boxplot(data, intervals, ttft_configs)"
      ],
      "metadata": {
        "id": "867QczQyUD4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Measure end-to-end latency."
      ],
      "metadata": {
        "id": "9TVieaVd2NfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if measure_latency:\n",
        "  latencies_e2e = {}\n",
        "  for idx in tqdm(indices):\n",
        "    random = dataset['train'].select([idx])\n",
        "    e2e_warmup = test_latency(random, iterations=iterations, warmup=True, max_new_tokens=256)\n",
        "    latencies_e2e[idx] = {\"warmup\": e2e_warmup}"
      ],
      "metadata": {
        "id": "CKlmGF2a64qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "for i,v in latencies_e2e.items():\n",
        "  data.append(v['warmup'])"
      ],
      "metadata": {
        "id": "mgIYZnvgN1RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the e2e latencies."
      ],
      "metadata": {
        "id": "h9amf3EAVVWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "e2e_configs = {}\n",
        "e2e_configs['title'] = 'Gemini 1B 4bit Latency for End-to-End (E2E) Completion'\n",
        "e2e_configs['xlabel'] = f\"Prompt Tokens with {iterations} Trials\"\n",
        "e2e_configs['ylabel'] = 'Latency (seconds)'\n",
        "e2e_configs['boxprops'] = dict(facecolor=\"tab:purple\")\n",
        "e2e_configs['png'] = \"e2e.png\""
      ],
      "metadata": {
        "id": "6srkV_uqUs-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_boxplot(data, intervals, e2e_configs)"
      ],
      "metadata": {
        "id": "MKFK0J75VirC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vq914BK1I3SC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}